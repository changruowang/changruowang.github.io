<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"xxxxx.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="可乐">
<meta property="og:type" content="website">
<meta property="og:title" content="Ruowang&#39;s blogs">
<meta property="og:url" content="http://xxxxx.com/page/10/index.html">
<meta property="og:site_name" content="Ruowang&#39;s blogs">
<meta property="og:description" content="可乐">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="常若望">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://xxxxx.com/page/10/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Ruowang's blogs</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta custom-logo">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Ruowang's blogs</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">去更远的地方，见更亮的光</p>
      <a>
        <img class="custom-logo-image" src="/images/icon.png" alt="Ruowang's blogs">
      </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://xxxxx.com/posts/32828/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="常若望">
      <meta itemprop="description" content="可乐">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ruowang's blogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/posts/32828/" class="post-title-link" itemprop="url">HDR-Net</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-05-11 10:10:35" itemprop="dateCreated datePublished" datetime="2021-05-11T10:10:35+08:00">2021-05-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-08-28 10:39:59" itemprop="dateModified" datetime="2021-08-28T10:39:59+08:00">2021-08-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%9B%BE%E5%83%8F%E9%A2%84%E5%A4%84%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">图像预处理</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%9B%BE%E5%83%8F%E9%A2%84%E5%A4%84%E7%90%86/%E5%9B%BE%E5%83%8F%E8%A7%86%E9%A2%91%E5%A2%9E%E5%BC%BA/" itemprop="url" rel="index"><span itemprop="name">图像视频增强</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>
<p>Deep Bilateral Learning for Real-Time Image Enhancement (HDR-Net)</p>
<p>Deep Bilateral Retinex for Low-Light Image Enhancement</p>
<h2 id="主要思想">主要思想</h2>
<p>借鉴了 双边网格（在快速双边滤波算法中被提出用于加速双边滤波）的思想 和 局部颜色仿射不变性的 特点，将图像缩放至低分辨率 输入网络，学习局部和全局特征，融合之后转换到双边网格中，得到双边网格下的局部仿射变换参数。并行的，对输入图像做仿射变换得到引导图，并将其用来引导前面的双边网格做空间和颜色深度上的插值，恢复到和原来图像一样大小的变换参数。最后根据这个参数对输入图像做仿射变换，得到输出图像。</p>
<h3 id="特点">特点</h3>
<ol type="1">
<li>大部分计算过程是在低分辨率的网格下进行的 - CNN中的局部和全局特征提取都是在低分辨率下执行。</li>
<li>学习的是输入输出的变换矩阵，而不是直接学习输出</li>
<li>虽然主要网络实在低分辨率下进行的 但是损失函数是在原来的分辨率上建立的，从而使得低分辨下的操作去优化原分辨下的图像。</li>
</ol>
<h2 id="主要内容">主要内容</h2>
<p>这篇文章主要是在先前的基础上进一步改进的，包括联合双边上采样（JBU)，这里是通过将双边滤波器作用在高分辨的引导图去产生局部平滑但是也保留边缘的上采样；双边引导上采样（BGU Bilateral Guided Upsampling ）则是引入了在双边网格里进行局部仿射变换，再通过引导图进行上采样。这篇论文实际上就是将BGU里的仿射变换操作通过网络进行学习。</p>
<h3 id="bgu-主要思想">BGU 主要思想</h3>
<p>文章提出了一种加速图像处理的方法。由于很多复杂的滤镜处理速度比较慢，一个很常用的解决思路是对原图 downsample 之后做处理，然后用 upsample 得到处理结果。而在 BGU 这个例子里，利用 bilateral grid 来做 downsample - upsample 的工作，使得效果更为出色。</p>
<figure>
<img src="https://cdn.jsdelivr.net/gh/changruowang/cloudimg/img/20210508212354.png" alt="" /><figcaption>1595208512974</figcaption>
</figure>
<ul>
<li>任何滤镜效果，在双边网格的局部小区域内（<strong>空域xy小范围内，以及像素域小范围内</strong>）都可以看做是一个线性变换。</li>
<li>利用 bilateral grid 可以从一个低分辨率的图上 slice 得到高分辨率的结果</li>
<li>upsample 针对的是变换系数，而不是直接针对像素。这样对细节方面损失降低到最小。</li>
</ul>
<p>具体实现步骤如下：</p>
<ol type="1">
<li>对原图 downsample 得到一个小图</li>
<li>在小图上应用滤镜</li>
<li>在小图上划分网格（bilateral graid），拟合每一个网格中的线性变换</li>
<li>线性变换的系数在网格间做平滑（这个平滑不仅在 x y 空间域的平滑，还在像素域z轴平滑，所以才要用双边网格，3D双边网格的作用就是以灰度值做第三维，将灰度差异在x轴上体现）</li>
<li>利用这个网格，根据原始大图在这个网格上做 slicing，得到高分辨率的线性变换系数，进一步得到高分辨率的结果</li>
</ol>
<h3 id="网络的主要结构">网络的主要结构：</h3>
<figure>
<img src="https://cdn.jsdelivr.net/gh/changruowang/cloudimg/img/20210508212421.png" alt="" /><figcaption>1595165752929</figcaption>
</figure>
<p>​ <strong>Low-level特征</strong> 首先将输入图像下采样至固定的256x256。然后一组共用的特征提取层，一共四层，每层为步长为2的3x3卷积和激活层。如果这个层数太少缺乏表达力，如下图对比，层数太多后面得到的仿射变换系数太稀疏（决定了双边网格的z轴的bin的数量)。</p>
<figure>
<img src="https://cdn.jsdelivr.net/gh/changruowang/cloudimg/img/20210508212416.png" alt="" /><figcaption>1595165304189</figcaption>
</figure>
<p>​ <strong>局部特征提</strong>取有两层卷积，这两个卷积都不改变feature map的尺寸，如果没有局部特征提取层，最后的预测的变换系数会失去空间信息。</p>
<p>​ <strong>全局特征提取</strong> 的支路包含了两个步长为2的卷积层和3个全连接层，最后输出一个包含全局信息的64维的特征向量。网络的输入图在提取特征时已经Resize成256x256了，所以可以直接用全连接。局特征具有的全局信息可以作为局部特征提取的先验，如果没有全局特征去描述图像信息的高维表示，网络可能会做出错误的局部特征表示，从而出现如上图的artifact。</p>
<p><strong>两个特征融合</strong> 论文的公式没看懂，但是看代码就是将来给你个特征直接相加 接一个relu再使用卷积层将维度转换为最终的96。具体的这部分卷积的尺寸如表.</p>
<h3 id="双边网格">双边网格</h3>
<p>​ 前面CNN最终输出的尺寸 为16X16X96 96 = 3X4X8; 输出图像为3通道，对应这里的3，4是每个输出通道的每个像素需要四个系数，三个对应输入图像的三通道值和一个偏移。也就是输出图像的每个像素位置需要一个3x4的变换矩阵。那么8就代表像素域的 bin 的数量。而空间域的bin的数量由输入图像和16的比值决定的。</p>
<p><strong>guide map</strong> 的 分辨率和原图一样，通道数为1 ，由原图通过几个卷积生成。</p>
<p><strong>使用可训练的slicing layer进行上采样 </strong> 这一步是要将上一步的信息转换到输入的高分辨率空间，这步操作基于双边网格里的slicing操作，通过一个单通道的引导图将A进行上采样。利用引导图g对A进行上采样，是利用A的系数进行三次线性插值，位置由g决定： <span class="math display">\[
\bar{A}_{c}[x, y]=\sum_{i, j, k} \tau\left(s_{x} x-i\right) \tau\left(s_{y} y-j\right) \tau(d \cdot g[x, y]-k) A_{c}[i, j, k]
\]</span> 这里 <span class="math display">\[
\tau(.)=\max (1-|\cdot|, 0)
\]</span>表示线性插值，<span class="math inline">\(s_{x}\)</span> <span class="math inline">\(s_{y}\)</span>表示网格的宽度和原图分辨率的长宽比。x 和 y 的位置由这两个长宽比决定其在网格中的对应位置，而我们知道网格z轴的 bin数量是8，应该是将z的8维度插值为 256bins 然后将bin合并成1 那么这里输出图像是 <span class="math inline">\(\bar{A}_{c}\)</span> 的 z 轴在网格对应的深度由guide map决定 即<span class="math inline">\(\bar{A}_{c}[i,j,g[x,y]]\)</span>，这个guide map是网络可训练的，那么最后每个<span class="math inline">\(\bar{A}_{c}\)</span> 像素的颜色深度也就由参与guide map决定，例如guide map上相邻灰度差异很大的像素，那么他们在原始网格也中映射的也是z轴上相距很远的两个bin，而BGU中说网格间是局部平滑，也即i索引的这两个变换矩阵差异会很大。但是这里是基于CNN的 并像BGU中那样直接对网格间的参数做平滑约束，这里就靠数据自己学习吧，最终学出来的也应该会有这个效果。 我感觉 它直接拿原图的灰度版本作为guide map 来指导插值也可以，但是这样相当于固定死了，原图差异多大的灰度，映射到网格中就是固定位置的bins虽然说也合理，但是 使用几层CNN来生成guide 就可学习更灵活了。这中以全分辨率的guide指导上采样 比直接使用 可学习的转置卷积上采样的对比。<strong>与基于转置卷积不同，这种方法在guide map的指导下可以很好的保留图像的边缘。</strong></p>
<figure>
<img src="https://cdn.jsdelivr.net/gh/changruowang/cloudimg/img/20210508212434.png" alt="" /><figcaption>1595211122459</figcaption>
</figure>
<h3 id="获得最终的输出">获得最终的输出</h3>
<p>​ 这一部分和上一部分中的guide map的计算是在全分辨率下进行的。这一步就是将上一步得到的全分辨率的变换矩阵（w x h x 12）用来对原图做变换。公式如下：</p>
<p><span class="math display">\[
\mathrm{O}_{c}[x, y]=\bar{A}_{n_{\phi}+\left(n_{\phi}+1\right) c}+\sum_{c^{\prime}=0}^{n_{\phi}-1} \bar{A}_{c^{\prime}+\left(n_{\phi}+1\right) c}[x, y] \phi_{c^{\prime}}[x, y]
\]</span></p>
<p>其中 <span class="math inline">\(n_{\phi}\)</span> = 3 表示输入图像的通道数，<span class="math inline">\(\phi_{c} = I\)</span>表示输入图像，输出的 <span class="math inline">\(\bar{A}\)</span> 为wxhx12的变换参数 w h 代表图像原图分辨率，12 = 3x4 按照按照 [R R R b1 G G G b2 B B B b3] 的顺序排列 R R R b1意味用于计算输出图像R通道值需要用的四个参数 且 公式中的下标按照feature 的通道序号索引的。 例如输出图像的r通道的某位置的值由 r(out)(x) = [a1, a2, a3] * [r, g, b]‘(input) + instance</p>
<h3 id="损失函数">损失函数</h3>
<p>训练参考图像为人工PS的参考图像，和网络生成的图像做损失即可。由此可见损失的计算是在全分辨率下完成的。</p>
<h2 id="实验">实验</h2>
<p>缺点，这个方法对于其他任务 例如图像去雾 ，深度估计，色彩化等任务上效果较差，这是因为其有较强的假设即输出是由输入的局部仿射变换得到的。 可以通过对输入图做特征进一步的提取特征来增强其表达效果。例如一个网格里使用36个仿射变换系数作用在一个层级为3的高斯金字塔处理的输入图要比原始的bilateral效果更好，尽管速度会变慢</p>
<p>….</p>
<h2 id="主要类容">主要类容</h2>
<p>这篇文章将 HDR中使用到的双边滤波的思想 和 Retinex 结合，来做图像增强。首先和HDR一样，将原始降采样，在低分辨率下进行 变换参数的估计。前半部分和HDR 完全一样，包括 CNN的设计 (全局和局部特征提取，guide map的设计)。只是输出的变换参数 维度为 wxhx(9 + 9x2 + 3x4) 这里 9 + 9x2 为噪声估计用到的变换参数，9是W 9x2是偏移量，相当于可变性卷积的意思；3x4是用于光照图估计的变化参数，和HDR中的方式一样。通过两个变换分别估计出了噪声图和光照图，使用式 <span class="math inline">\(\widetilde{\boldsymbol{R}}=(\boldsymbol{I}-\boldsymbol{N}) \oslash \boldsymbol{E}\)</span> 估计最终增强之后的图像。</p>
<h3 id="网络的主要结构-1">网络的主要结构</h3>
<figure>
<img src="https://cdn.jsdelivr.net/gh/changruowang/cloudimg/img/20210508212440.png" alt="" /><figcaption>1595218108228</figcaption>
</figure>
<h3 id="噪声图的估计">噪声图的估计</h3>
<p>根据 变换参数对输入图像变换得到噪声估计图像，变换参数为 9 + 9 x 2 感觉相当于1X1可变性卷积。对于某个像素位置 输入为三通道 输出也为三通道，相当于需要 3x3个1x1卷积核，而 9x2 为x y两个方向上的偏移，即可变性卷积的原理。</p>
<ul class="task-list">
<li><input type="checkbox" disabled="" />
最后的噪声变换 是否为 1x1的可变形卷积？</li>
</ul>
<h3 id="损失函数-1">损失函数</h3>
<p>作者使用 LOL 数据集训练 LOL 包含了 1500 low/noemal 图像对，其中500对是真实数据 其他的为合成数据。这里我饿认为LOL提供的不算参考图吧 只是 一对儿不同曝光度的图像。但是作者直接将high作为参考图像来构建损失函数。 <span class="math display">\[
\mathcal{L}:=\mathcal{L}_{r}(\boldsymbol{R}, \tilde{\boldsymbol{R}})+\lambda_{n} \mathcal{L}_{n}(\boldsymbol{N})+\lambda_{e} \mathcal{L}_{e}(\boldsymbol{E}, \boldsymbol{I})
\]</span> 第一项即估计的R和参考的R的相似度，具体不仅包含了衡量两个R的相似度的L1损失还有两个梯度相似度的L1损失。第二项损失用来尽量保存图像中的边缘。第三项即常规光照平滑损失。</p>
<h3 id="实现细节">实现细节</h3>
<p>​ 在训练的时候将输入图像归一化至[0,1] crop到 256x256 batch size 设置为16可变形卷积设置的K=3 Window size=15 边长缩放比例为16 32</p>
<h3 id="结果">结果</h3>
<p>​</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://xxxxx.com/posts/26067/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="常若望">
      <meta itemprop="description" content="可乐">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ruowang's blogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/posts/26067/" class="post-title-link" itemprop="url">Frequence-Decomposition</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-05-11 10:10:34" itemprop="dateCreated datePublished" datetime="2021-05-11T10:10:34+08:00">2021-05-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-08-28 10:39:51" itemprop="dateModified" datetime="2021-08-28T10:39:51+08:00">2021-08-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%9B%BE%E5%83%8F%E9%A2%84%E5%A4%84%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">图像预处理</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%9B%BE%E5%83%8F%E9%A2%84%E5%A4%84%E7%90%86/%E5%9B%BE%E5%83%8F%E8%A7%86%E9%A2%91%E5%A2%9E%E5%BC%BA/" itemprop="url" rel="index"><span itemprop="name">图像视频增强</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>
<p>Learning to Restore Low-Light Images via Decomposition-and-Enhancement CVPR2020</p>
<h2 id="思考">思考</h2>
<p>文章主要思想是 频率分解，是个新思路 但是没有在无参考图像上的对比实验，且对比的方法比较旧。文章也使用了VGG损失且确实会有效果。这种分解的思路可以借鉴，对参考图像做处理来指导网络学习期望的效果。</p>
<h2 id="主要思想">主要思想</h2>
<p>​ 作者发现 <strong>低光照图像的噪声在不同频率的层会表现出不同的对比度，在低频层中比在高频层中更容易检测到噪声。</strong>基于此思想，提出了一个基于频率分解-增强的两阶段低光照图像增强模型。也是从粗糙到精细coarse to fine的思路。同时 还提出一个新的包含真实噪声的数据集。如下图：</p>
<figure>
<img src="https://cdn.jsdelivr.net/gh/changruowang/cloudimg/img/20210424120217.png" alt="" /><figcaption>1598948440726</figcaption>
</figure>
<h2 id="主要内容">主要内容</h2>
<figure>
<img src="https://cdn.jsdelivr.net/gh/changruowang/cloudimg/img/20210424120223.png" alt="" /><figcaption>1598949161921</figcaption>
</figure>
<h3 id="整体结构">整体结构</h3>
<p>​ 首先，与直接增强整个图像相比，增强 含有噪声的低光图像的低频层更容易。这是因为低频层的噪声更容易检测和抑制。通过分析图像低频层的全局属性，可以正确地估计图像的光照/颜色；并且图像的边缘或者角点只占图像的很低维度(?)因此，给定基础的低频信息，可以推断出相应的高频信息。</p>
<p>​ 第一阶段，输入图像，学习一个可以获得图像低频信息的网络 获取 增强后的低频图像 content map C(.)，过滤掉了高频信息。然后使用放大函数 A(.) 用作颜色恢复和进一步增强。具体的：</p>
<p><span class="math display">\[I^{a}=\alpha A(C(I)) \cdot C(I)\]</span></p>
<p><span class="math inline">\(I^{a}\)</span>是放大后的低频图像，这里并非retinex的那种illumination map的增强，而是类似于一种attention <span class="math inline">\(\alpha\)</span>也是和C用一个网络生成的。</p>
<p>​ 第二阶段，就是根据第一阶段的<span class="math inline">\(I^{a}\)</span>来恢复高频细节。 第一阶段C的 监督 是 参考图像经过 指导滤波后的图像（详情见损失部分）第二阶段的 监督就是正常参考图。这一部分用了一个残差结构。</p>
<p><span class="math display">\[I^{c}=I^{a}+D\left(I^{a}\right)\]</span></p>
<h3 id="ace模块">ACE模块</h3>
<p>​ 又总的结构图可以看出 在第一第二阶段输入的时候都经过了一个ACE模块，这个模块就是起到筛选 低频/高频 成分的功能。后面接的对应网络完成 低频/高频 图像 中的噪声去除和细节恢复。ACE结构如下：</p>
<figure>
<img src="https://cdn.jsdelivr.net/gh/changruowang/cloudimg/img/20210424120229.png" alt="" /><figcaption>1598951252434</figcaption>
</figure>
<p>ACE 对输入图像做了两个并列的 不同 空洞率的卷积 ，然后做了 <strong>差</strong> 生成了Ca <span class="math inline">\(C_{a}=\operatorname{sigmoid}\left(f_{d 1}\left(x_{i n}\right)-f_{d 2}\left(x_{i n}\right)\right)\)</span> Ca 表明 像素间的相对差异，差异大的就是高频区域 反之低频。将Ca作为一个全图的权重 和 输入特征x_in点乘，来过滤高低频信息（1-Ca） 再后面的部分就是 nonlocal 模块了，作者这里进行了降采样 提高计算速度。这里在第一个图中的两个ACE是共享权重的。</p>
<h3 id="cdt模块">CDT模块</h3>
<p>同ACE DCT首先接入的也是 1-Ca 作为引导滤出高频区域。然后使用self attention生产vector进行channel 的缩放，就是通道注意力模块。CDT模块是为了减小输入特征和增强特征的差距 并 扩大感受野。</p>
<h3 id="损失函数">损失函数</h3>
<p>​ 损失包含了以下三项。其中 C是第一个阶段的content图，GT的指导滤波输出的低频图，最终的增强图和GT图。fi是vgg网络输出的损失。</p>
<p><span class="math display">\[L_{a c c}=\lambda_{1}\left\|C-I_{f}^{g t}\right\|_{2}+\lambda_{2}\left\|I^{c}-I^{g t}\right\|_{2}\]</span></p>
<p><span class="math inline">\(L_{v g g}=\lambda_{3}\left\|\Phi\left(I^{c}\right)-\Phi\left(I^{g t}\right)\right\|_{1}\)</span></p>
<h2 id="实验">实验</h2>
<h3 id="实现细节">实现细节</h3>
<p>​ 第一阶段中的 <span class="math inline">\(\alpha\)</span> 初始设置为 1 。训练图像随机裁剪到 512X384。分辨率为1024x768的输入图像处理耗时0.33s 在1080显卡上。</p>
<h3 id="对比实验">对比实验</h3>
<figure>
<img src="https://cdn.jsdelivr.net/gh/changruowang/cloudimg/img/20210424120236.png" alt="" /><figcaption>1598952274964</figcaption>
</figure>
<p>一个是 对 网络中的各个模块 ACE DCT等做了对比实验，另一个是 在自己训练用的RGB数据集上和其他发给发做了对比，再就是对 加入去噪流程的其他方法做了对比。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://xxxxx.com/posts/9641/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="常若望">
      <meta itemprop="description" content="可乐">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ruowang's blogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/posts/9641/" class="post-title-link" itemprop="url">Extreme-Low-light</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-05-11 10:10:33" itemprop="dateCreated datePublished" datetime="2021-05-11T10:10:33+08:00">2021-05-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-08-27 17:00:31" itemprop="dateModified" datetime="2021-08-27T17:00:31+08:00">2021-08-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%9B%BE%E5%83%8F%E9%A2%84%E5%A4%84%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">图像预处理</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%9B%BE%E5%83%8F%E9%A2%84%E5%A4%84%E7%90%86/%E5%9B%BE%E5%83%8F%E8%A7%86%E9%A2%91%E5%A2%9E%E5%BC%BA/" itemprop="url" rel="index"><span itemprop="name">图像视频增强</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://xxxxx.com/posts/9266/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="常若望">
      <meta itemprop="description" content="可乐">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ruowang's blogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/posts/9266/" class="post-title-link" itemprop="url">EnlightenGAN</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-05-11 10:10:32" itemprop="dateCreated datePublished" datetime="2021-05-11T10:10:32+08:00">2021-05-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-08-28 10:39:42" itemprop="dateModified" datetime="2021-08-28T10:39:42+08:00">2021-08-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%9B%BE%E5%83%8F%E9%A2%84%E5%A4%84%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">图像预处理</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%9B%BE%E5%83%8F%E9%A2%84%E5%A4%84%E7%90%86/%E5%9B%BE%E5%83%8F%E8%A7%86%E9%A2%91%E5%A2%9E%E5%BC%BA/" itemprop="url" rel="index"><span itemprop="name">图像视频增强</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>
<p>想解决的问题：低光照图像增强</p>
<p>方法：</p>
<ul>
<li>提出非监督的生成对抗网络，EnlightenGAN，训练不需要低光度/正常光度图像对。</li>
<li>提出使用从输入本身提取的信息来规范化不成对的训练</li>
<li>一些创新：
<ul>
<li>global-local判断器结构</li>
<li>self-regularized perceptual loss fusion</li>
<li>注意力机制</li>
</ul></li>
</ul>
<h2 id="introduction">Introduction</h2>
<p>DL方法通常用成对图像作为输入，<strong>缺点</strong>：</p>
<ol type="1">
<li>很难在实际中同时采集到同一场景的图像</li>
<li>合成的图像不够真实，引入了许多人为因素</li>
<li>对于低光照问题，没有唯一的或者说定义好的GT</li>
</ol>
<p>解决的问题：</p>
<ul>
<li>对低光照图像进行增强，但不需要成对的训练数据。</li>
</ul>
<p>idea：</p>
<ul>
<li>使用GAN在low和normal light之间建立unpaired mapping，但不依赖任何成对的图像</li>
<li>没有使用cycle-consistency作为prior工作
<ul>
<li>（不懂这个cycle-consistency是什么）</li>
<li>ref：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/70592331">参考资料</a></li>
<li>一些point：
<ul>
<li>最早是在朱俊彦的cycleGAN提出，用于在没有配对数据的情况下实现2个domain的image-to-image翻译。基本的思想，假设从X翻译到Y得到F(x)，再翻译回去G(F(x))，G(F(x))应该与X一模一样。</li>
<li></li>
</ul></li>
</ul></li>
<li>一些创新性工作
<ul>
<li>1.提出dual-discriminator平衡全局和局部低光照增强x</li>
<li>2.提出self-regularized perceptual loss来约束输入和增强版本后的特征距离</li>
<li>3.提出开发的输入的光照信息作为self-regularized attentional map，在每个level的深度特征上regularize无监督学习</li>
</ul></li>
</ul>
<p>EnlightenGAN的创新点：</p>
<ol type="1">
<li>第一个将unpaired训练引入低光照增强</li>
<li>self-regularization，通过保留自特征loss和自规则注意力机制实现</li>
<li>增强来自不同domain的真实世界的低光照图像更加简单和灵活</li>
</ol>
<h2 id="related-works">Related Works</h2>
<h3 id="paired-datasetsstatus-quo"><strong>Paired Datasets：Status Quo</strong></h3>
<p>缺点：</p>
<ul>
<li>数据量小，简单地增加或减少曝光时间会增加或减少局部的曝光。</li>
<li>在HDR领域，一些工作首先获取不同光照条件下的图像，然后对其进行排列和融合，但他们不是为单一图像后处理而设计的</li>
</ul>
<h3 id="traditional-approaches">Traditional Approaches</h3>
<p>经典方法：</p>
<ul>
<li>adaptive histogram equalization（AHE），自适应直方图平衡
<ul>
<li>Stephen M Pizer, E Philip Amburn, John D Austin, Robert Cromartie, Ari Geselowitz, Trey Greer, Bart ter Haar Romeny, John B Zimmerman, and Karel Zuiderveld. Adaptive histogram equalization and its variations. Com- puter vision, graphics, and image processing, 39(3):355– 368, <strong>1987.</strong></li>
</ul></li>
<li>Retinex
<ul>
<li>Edwin H Land. The retinex theory of color vision. Scientific american, 237(6):108–129, <strong>1977</strong></li>
</ul></li>
<li>multi-scale Retinex model
<ul>
<li>Daniel J Jobson, Zia-ur Rahman, and Glenn A Woodell. A multiscale retinex for bridging the gap between color images and the human observation of scenes. IEEE Transactions on Image processing, 6(7):965–976, <strong>1997</strong></li>
</ul></li>
<li>提出针对不均匀光照用bi-log信息平衡细节与自然感的增强算法
<ul>
<li>Shuhang Wang, Jin Zheng, Hai-Miao Hu, and Bo Li. Nat- uralness preserved enhancement algorithm for non-uniform illumination images. IEEE Transactions on Image Process- ing, 22(9):3538–3548, <strong>2013</strong></li>
</ul></li>
<li>提出加权变分模型，估计reflectance和illumination
<ul>
<li>Xueyang Fu, Delu Zeng, Yue Huang, Xiao-Ping Zhang, and Xinghao Ding. A weighted variational model for simultane- ous reflectance and illumination estimation. In CVPR, pages 2782–2790, <strong>2016</strong></li>
</ul></li>
<li>LIME，low-light image enhancement，先找到RGB中的最大值作为最初的光照估计，使用结构先验构建光照图
<ul>
<li>Xiaojie Guo, Yu Li, and Haibin Ling. Lime: Low-light im- age enhancement via illumination map estimation. IEEE Transactions on Image Processing, 26(2):982–993, <strong>2017</strong></li>
</ul></li>
<li>通过分解连续图像序列来同时处理低光照和去噪
<ul>
<li>Xutong Ren, Mading Li, Wen-Huang Cheng, and Jiaying Liu. Joint enhancement and denoising method via sequen-tial decomposition. In Circuits and Systems (ISCAS), 2018 IEEE International Symposium on, pages 1–5. IEEE, <strong>2018</strong></li>
</ul></li>
<li>提出更加robust的Retinex模型，与传统的Retinex模型对比，考虑了noise map，通过强噪声提高性能
<ul>
<li>Mading Li, Jiaying Liu, Wenhan Yang, Xiaoyan Sun, and Zongming Guo. Structure-revealing low-light image en- hancement via robust retinex model. IEEE Transactions on Image Processing, 27(6):2828–2841, <strong>2018.</strong></li>
</ul></li>
</ul>
<h3 id="deep-learning-approaches">Deep Learning Approaches</h3>
<p>目前大部分基于DL的方法都依赖于paired image，且图像大部分是从正常图像中人工合成的。</p>
<ul>
<li>LL-Net，堆叠的自动编码器，在patch level同时学习去噪和低光照增强。
<ul>
<li>Kin Gwn Lore, Adedotun Akintayo, and Soumik Sarkar. Ll- net: A deep autoencoder approach to natural low-light image enhancement. Pattern Recognition, 61:650–662, <strong>2017</strong>.</li>
</ul></li>
<li>Retinex-Net，设计了end-to-end框架，结合了Retinex理论
<ul>
<li>Chen Wei, Wenjing Wang, Wenhan Yang, and Jiaying Liu. Deep retinex decomposition for low-light enhancement. arXiv preprint arXiv:1808.04560, <strong>2018</strong>.</li>
</ul></li>
<li>HDR-Net ，将深度网络与双边网格处理（bilateral gird processing）、局部颜色仿射变换（local affine color transforms）
<ul>
<li>Micha¨el Gharbi, Jiawen Chen, Jonathan T Barron, SamuelW Hasinoff, and Fr´edo Durand. Deep bilateral learning for real- time image enhancement. ACM Transactions on Graphics (TOG), 36(4):118, <strong>2017</strong></li>
</ul></li>
<li>以及一些针对HDR领域的多帧低光照增强方法
<ul>
<li>Nima Khademi Kalantari and Ravi Ramamoorthi. Deep high dynamic range imaging of dynamic scenes. ACM Trans. Graph, 36(4):144, <strong>2017</strong>.</li>
<li>ShangzheWu, Jiarui Xu, Yu-Wing Tai, and Chi-Keung Tang. Deep high dynamic range imaging with large foreground motions. In Proceedings of the European Conference on Computer Vision (ECCV), pages 117–132, <strong>2018</strong>.</li>
<li>Jianrui Cai, Shuhang Gu, and Lei Zhang. Learning a deep single image contrast enhancer from multi-exposure images. IEEE Transactions on Image Processing, 27(4):2049–2062, <strong>2018</strong>.</li>
</ul></li>
<li>learning to see in the dark，直接在raw数据上，更注重避开放大的artifacts</li>
</ul>
<h3 id="adversarial-learning">Adversarial Learning</h3>
<p>使用GAN的方法同样使用的是paired训练数据。</p>
<p>一些人提出了无监督的GAN方法，使用对抗学习学习inter-domain。</p>
<p>提出两个方法，通过使用cycle-consistent loss+uppaired data，对两个不同领域之间进行翻译</p>
<ul>
<li>Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle- consistent adversarial networks. In ICCV, pages 2223–2232, <strong>2017</strong>.</li>
<li>Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks. In Advances in Neural Information Processing Systems, pages 700–708, <strong>2017</strong>.</li>
</ul>
<p>一些最新的工作基于上面的方法论应用在其他low level task（比如，去雾，去噪，SR，手机照片增强等）上：</p>
<ul>
<li>Xitong Yang, Zheng Xu, and Jiebo Luo. Towards percep tual image dehazing by physics-based disentanglement and adversarial training. In The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18), <strong>2018</strong>.</li>
<li>Yuan Yuan, Siyuan Liu, Jiawei Zhang, Yongbing Zhang, Chao Dong, and Liang Lin. Unsupervised image super- resolution using cycle-in-cycle generative adversarial net- works. CVPR Workshops, 30:32, <strong>2018</strong>.</li>
<li>Yu-Sheng Chen, Yu-Ching Wang, Man-Hsin Kao, and Yung- Yu Chuang. Deep photo enhancer: Unpaired learning for image enhancement from photographs with gans. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6306–6314, <strong>2018</strong>.</li>
<li>Xin Jin, Zhibo Chen, Jianxin Lin, Zhikai Chen, and Wei Zhou. Unsupervised single image deraining with self- supervised constraints. arXiv preprint arXiv:1811.08575, <strong>2018</strong>.</li>
</ul>
<p>EnlightenGAN采用unpaired训练，但是一个轻量级的one-pathGAN结构（即，没有cycle-consistency）,这样的好处是，训练稳定且简单。</p>
<h2 id="method">Method</h2>
<p>网络结构：</p>
<p><a target="_blank" rel="noopener" href="https://github.com/GlassyWu/Note/blob/master/Paper/低光照图像增强/img/EnlightenGAN.png"><img src="https://github.com/GlassyWu/Note/raw/master/Paper/%E4%BD%8E%E5%85%89%E7%85%A7%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA/img/EnlightenGAN.png" alt="img" /></a></p>
<ul>
<li>采用一个attention-guided U-Net作为生成器</li>
<li>用对偶的判断器判断全局和局部信息（即，一个global和一个local判断器）</li>
<li>使用一个自特征保留loss来引导训练，以及保留纹理和结构</li>
</ul>
<h4 id="global-local-discriminators">（1）Global-Local Discriminators</h4>
<p>用对抗loss来最小化真实和输出的正常光线的分布之间的距离。</p>
<p>image-level的判断器经常在空间变化的光照图像不work，在一些局部区域需要增强的部分和其他区域不太一样（比如，全局大部分是暗的，一个小部分是亮的，这种情况全局生成器可能就不能满足这个需求）。</p>
<p>解决方法：</p>
<p>作者设计了一个global-local判断器结构，都是用PatchGAN来判断真假。</p>
<ul>
<li><p>local判断器：</p>
<ul>
<li>在输出和正常光线图像随机裁剪局部patch，判断器学习判断这些patch的真假。</li>
</ul></li>
<li><p>global判断器：</p>
<ul>
<li>使用了相对判断器结构
<ul>
<li>relativistic discriminator structure：
<ul>
<li>Alexia Jolicoeur-Martineau. The relativistic discriminator: a key element missing from standard gan. arXiv preprint arXiv:1807.00734, 2018</li>
</ul></li>
</ul></li>
<li>在标准的相对判断器上将sigmoid函数改为最小二乘法loss</li>
</ul>
<p>标准相对判断器公式： <span class="math display">\[ D_{Ra}(x_{r},x_{f})=σ(C(x_{r})-E_{x_{f}∼P_{fake}}[C(x_{f})]) \ D_{Ra}(x_{f},x_{r})=σ(C(x_{f})-E_{x_{f}∼P_{real}}[C(x_{r})]) \]</span> 作者进行修改后的loss：</p>
<p><strong>Global：</strong> <span class="math display">\[ L_{D}^{Global}=E_{X_{r}∼P_{real}}[(D_{Ra}(x_{r},x_{f})-1)^2]+E_{x_{f}∼P_{fake}}[D_{Ra}(x_f, x_r)^2] \ L_{G}^{Global}=E_{X_{f}∼P_{fake}}[(D_{Ra}(x_{f},x_{r})-1)^2]+E_{x_{r}∼P_{real}}[D_{Ra}(x_r, x_f)^2] \]</span> <strong>Local:</strong> <span class="math display">\[ L_{D}^{Local}=E_{x_{r}∼P_{real-patches}}[(D(x_{r})-1)^2]+E_{x_{f}∼P_{fake-patches}}[(D(x_{f})-0)^2] \ L_{G}^{Local}=E_{x_{r}∼P_{fake-patches}[(D(x_{f})-1)^2]} \]</span></p></li>
</ul>
<h4 id="self-feature-preserving-loss">（2）Self Feature Preserving Loss</h4>
<p>perceptual loss常用来限制提取的特征与GT尽可能接近，perceptual loss是利用预训练的VGG去模拟图像之间的特征空间距离。</p>
<p><strong>paper</strong>：Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In European Conference on Computer Vision, pages 694–711. Springer, <strong>2016</strong>.</p>
<p>作者提出限制输入和输出的VGG-frature距离。</p>
<p>作者根据经验观察到当调整输入像素的密度范围，VGG的分类结果不是特别灵敏。（也被论文证实了）</p>
<p>设计了自正则化loss（self-regularzation loss)，以保留图像内容特征。</p>
<p>L_SFP的定义：</p>
<p><a target="_blank" rel="noopener" href="https://github.com/GlassyWu/Note/blob/master/Paper/低光照图像增强/img/L_SFP.png"><img src="https://github.com/GlassyWu/Note/raw/master/Paper/%E4%BD%8E%E5%85%89%E7%85%A7%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA/img/L_SFP.png" alt="img" /></a></p>
<p>对于裁剪的局部块也进行规则，用L_SFP_Local，此外，还在VGG特征图之后增加了instance normalization，目的是稳定训练。</p>
<h4 id="u-net-generator-guided-with-self-regularized-attention">（3）U-Net Generator Guided with Self-Regularized Attention</h4>
<ul>
<li>U-Net作为backbone</li>
<li>将输入的RGB正则化到[0,1]，用1-I作为self-regularized attention map</li>
<li>然后resize attention map来适配每个feature map，然后与所有中间feature map、输出进行相乘</li>
<li>作者强调attention map也是self-regularization的一部分</li>
</ul>
<p>attention-guided U-Net generator结构：</p>
<ul>
<li>8个conv块 ，每个conv包括LeakyRelU+BN+2个3x3 conv</li>
<li>在上采样阶段，将标准deconvolutional layer替换为双线上采样层（bilinear upsampling）+一个卷积层，目的是减少checkerboard artifacts
<ul>
<li>(不懂bilinear upsampling，checkerboard artifacts)</li>
</ul></li>
</ul>
<h2 id="gan相关论文">GAN相关论文：</h2>
<p>1、Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle- consistent adversarial networks. In ICCV, pages 2223–2232, 2017.</p>
<p>2、Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks. In Advances in Neural Information Processing Systems, pages 700–708, 2017</p>
<p>3、Alexia Jolicoeur-Martineau. The relativistic discriminator: a key element missing from standard gan. arXiv preprint arXiv:1807.00734, 2018.</p>
<h2 id="其他">其他</h2>
<ul>
<li>HDR(high-dynamic-ranging)
<ul>
<li>高动态范围成像，用来实现比普通数字图像技术更大曝光动态范围（即，更大的明暗差别）的一组技术</li>
<li>目的：正确地表示真实世界中从太阳光直射到最暗的阴影这样大的范围亮度</li>
</ul></li>
<li>ablation analysis
<ul>
<li>消融实验：分析不同参数或结构对实验结果产生的影响从而得到不同成分的作用</li>
</ul></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://xxxxx.com/posts/46341/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="常若望">
      <meta itemprop="description" content="可乐">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ruowang's blogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/posts/46341/" class="post-title-link" itemprop="url">DRBN</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-05-11 10:10:31" itemprop="dateCreated datePublished" datetime="2021-05-11T10:10:31+08:00">2021-05-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-08-28 10:39:29" itemprop="dateModified" datetime="2021-08-28T10:39:29+08:00">2021-08-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%9B%BE%E5%83%8F%E9%A2%84%E5%A4%84%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">图像预处理</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%9B%BE%E5%83%8F%E9%A2%84%E5%A4%84%E7%90%86/%E5%9B%BE%E5%83%8F%E8%A7%86%E9%A2%91%E5%A2%9E%E5%BC%BA/" itemprop="url" rel="index"><span itemprop="name">图像视频增强</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>
<p>From Fidelity to Perceptual Quality: A Semi-Supervised Approach for Low-Light Image Enhancement CVPR2020</p>
<p>DRBN</p>
<h2 id="思考">思考</h2>
<p>值得注意的地方：</p>
<ol type="1">
<li>设计了 一个 递归的网络结构，结构比较有新意，第一阶段训练始终将特征分为三个分辨率频带，并且不同频带间使用残差学习的结构。</li>
<li>将对抗损失引入，用来提高 第一阶段 端到端学习生成图像饱和度 亮度等依然不足存在的问题。而这个对抗损失的训练又可以使用其他不配对的数据。</li>
</ol>
<p>疑问：</p>
<ol type="1">
<li>整个结构还是在学习低光照图像到 正常光照图像间的 端到端的映射，而且 以这个方式在LOL数据集上训练 最终在其他无参考图像数据上的泛化能力如何 还未知，文章也未做对比试验。所以感觉泛化能力不行，估计不如使用retinex的网络。</li>
<li>LOL数据集的成对图像本来就不适合这种端到端的学习，因为它的正常光照图像其实很多效果都不是很好(亮度不够，可能只是没有噪声)，因为这个数据集的主要目低时提供不同曝光度的图像对 用于光照估计分解。所以作者不得不在第二阶段加入 一个类似对比度增强的变换网络。</li>
<li>有待在其他数据集上验证效果。</li>
</ol>
<h2 id="主要贡献">主要贡献</h2>
<p>首先提出了一个深度(D)递归(Recursive)带(Band)网络(Network) 使用成对的 正常/低光照图像 去学习增强的正常光照图像的线性递归带表示。然后使用另一个网络来学习线性变换 提高上一阶段输出图像的视觉效果，此阶段基于 未配对的 感知质量驱动的对抗学习损失训练。</p>
<ul>
<li>这是首次 提出 适用于低光照图像增强任务的 半监督学习框架，设计了 深度递归带的表示形式，来连接全监督和无监督部分 以整合他们的优点。（什么是无监督 什么是半监督？）</li>
<li>提出的框架经过精心设计，可以提取一系<strong>列粗到细的频带</strong>表示。 通过以递归的方式进行端到端训练，能够消除噪声和校正细节，这些频带表示的估计<strong>是相互受益的</strong>。</li>
<li>在质量感知的对抗性学习的指导下，深层表示被重新变换。基于平均意见得分（MOS）在感知上选择鉴别器的“真实图像”。 低光图像增强任务中的第一个试验。</li>
</ul>
<h2 id="主要内容">主要内容</h2>
<h3 id="动机">动机</h3>
<p><strong>Recursive Band Learning</strong> paired 成对的图像数据 可以对图像细节增强提供强大的约束。所以第一阶段，基于 成对的训练数据的约束，使用 深度递归带 (这里的带 应该代表频带，表示不同分辨率的特征图/图象) 网络 来恢复重建图像的细节信号。这一阶段不仅从输入图像 y 中 生成了最终增强图像 <span class="math inline">\(\hat{x}=\sum_{i=1}^{n} \hat{x}_{s_{i}}^{T}\)</span> ，还生成了 一系列分辨率的特征带 <span class="math display">\[\left\{\Delta \hat{x}_{s_{1}}^{T}, \Delta \hat{x}_{s_{2}}^{T}, \ldots, \Delta \hat{x}_{s_{n}}^{T}\right\}\]</span> 通过对成对的低光/正常光数据进行完全监督来学习 <span class="math inline">\(\Delta \hat{x}_{s_{i}}^{T}\)</span> 。</p>
<p><strong>连接递归带的特征和对抗学习</strong>。但是，第一阶段的首要目的是尽可能恢复信号细节，自然无法获得良好的视觉质量。因此这一阶段对第一阶段学习的 信号带进行重建，来获得更符合人类视觉效果的更好的结果。如下式：</p>
<p><span class="math display">\[\hat{x}=\sum_{i=1}^{n} w_{i}\left(y,\left\{\Delta \hat{x}_{s_{1}}^{T}, \Delta \hat{x}_{s_{2}}^{T}, \ldots, \Delta \hat{x}_{s_{n}}^{T}\right\}\right) \Delta \hat{x}_{s_{i}}^{T}(y)\]</span></p>
<p>上式可以看出 ，首先这阶段的网络输入是 上一阶段生成的几乎无噪声的，细节很好的图像，输出获得具有更好光照更好对比图的图像。网络学习的是变换参数。</p>
<h3 id="网络结构">网络结构</h3>
<figure>
<img src="https://cdn.jsdelivr.net/gh/changruowang/cloudimg/img/20210508212517.png" alt="" /><figcaption>1598796721130</figcaption>
</figure>
<p><strong>Recursive Band Learning</strong></p>
<p>这一阶段使用一系列 U-NET类似的网络结构(BLN)来完成迭代。图中的分辨率分别为 1/4， 1/2，1 。首先输入图像y经过一个BLN_F 生成 。f 为每个网络生成的中间特征，有不同分辨率的 和上一轮生成的 f 求和 生成当前层的 x 输出，其次也会直接输入下一层 和下一层的 f 求和。这样以来 每次递归的其实是 学习的 上一轮 生成的 f 的残差。所以说由粗到精细特征。</p>
<p><span class="math display">\[\begin{aligned}
\left[f_{s_{1}}^{1}, f_{s_{2}}^{1}, f_{s_{3}}^{1}\right] &amp;=F_{\mathrm{BLN}_{-}}^{1}(y) \\
\hat{x}_{s_{1}}^{1} &amp;=F_{\mathrm{R}_{-s} 1}^{1}\left(f_{s_{1}}^{1}\right) \\
\hat{x}_{s_{2}}^{1} &amp;=F_{\mathrm{R}_{-s} 2}^{1}\left(f_{s_{2}}^{1}\right)+F_{\mathrm{U}}\left(\hat{x}_{s_{1}}^{1}\right) \\
\hat{x}_{s_{3}}^{1} &amp;=F_{\mathrm{R}_{s_{3}}}^{1}\left(f_{s_{3}}^{1}\right)+F_{\mathrm{U}}\left(\hat{x}_{s_{2}}^{1}\right)
\end{aligned}\]</span></p>
<p>接着是中间层的 不断 递归迭代。输入为 y 和上一层增强的输出。中间<strong>层的递归网络学习的都是前一层的 残差</strong></p>
<p><span class="math display">\[\begin{aligned}
\left[\Delta f_{s_{1}}^{t}, \Delta f_{s_{2}}^{t}, \Delta f_{s_{3}}^{t}\right] &amp;=F_{\mathrm{BLN}_{-} \mathrm{F}}^{t}\left(y, \hat{x}_{s_{3}}^{t-1}\right) \\
f_{s_{i}}^{t} &amp;=\Delta f_{s_{i}}^{t}+f_{s_{i}}^{t-1}, i=1,2,3 \\
\hat{x}_{s_{1}}^{t} &amp;=F_{\mathrm{R}_{s} s_{1}}^{t}\left(f_{s_{1}}^{t}\right) \\
\hat{x}_{s_{2}}^{t} &amp;=F_{\mathrm{R}_{s} s_{2}}^{t}\left(f_{s_{2}}^{t}\right)+F_{\mathrm{U}}\left(\hat{x}_{s_{1}}^{t}\right) \\
\hat{x}_{s_{3}}^{t} &amp;=F_{\mathrm{R}_{s} s_{3}}^{t}\left(f_{s_{3}}^{t}\right)+F_{\mathrm{U}}\left(\hat{x}_{s_{2}}^{t}\right)
\end{aligned}\]</span></p>
<p>最后是 损失函数。可以看出，它是 分别计算在分辨率 s=1 /2 1/4下的SSIM损失 FD为下采样。</p>
<p><span class="math display">\[\begin{aligned}
L_{\mathrm{Rect}}=-&amp;\left(\phi\left(\hat{x}_{s_{3}}^{T}, x\right)+\lambda_{1} \phi\left(\hat{x}_{s_{2}}^{T}, F_{D}\left(x, s_{2}\right)\right)\right.\\
&amp;\left.+\lambda_{2} \phi\left(\hat{x}_{s_{1}}^{T}, F_{D}\left(x, s_{1}\right)\right)\right)
\end{aligned}\]</span></p>
<p>上述的设计有以下好处：</p>
<ul>
<li>上一次迭代生成的高频带将会对这次迭代的低频特征的生成产生影响，因为这次迭代的输入是上一次生成的的高频带特征 S3 和 y的组合。而最终的损失也是高低频带分别计算，所以高低频带之间的连接是相互影响的，双向流动的。</li>
<li>递归学习增强了建模能力。 后面的重复仅需要恢复残差信号，并以先前重复的估计为指导。 因此，可以获得准确的估计，只需将更多的注意力放在细节的恢复上。</li>
</ul>
<p><strong>Band Recomposition</strong></p>
<p>借助配对数据的约束，可以很好地学习从弱光图像到正常光图像的波段恢复过程，同时可以很好地恢复细节并抑制噪声。 由于信号保真度始终无法很好地与人类的视觉感知保持一致，尤其是对于图像的某些全局属性（例如光线，颜色分布）。因此这一部分的目的就是 对上一阶段的频带信号重建，得到视觉效果更好的图像。</p>
<p>首先 可以看出 左半部分还是使用一个UNet结构的网络来学习变换的参数，使用生成的参数对 原始频带型号线性变换。得到新的生成图像，使用 对抗损失 和 SSIM/VGG 损失共同优化网络。而对抗损失 是使用 另一个数据集学习的用于度量图像频分的一个网络 SSIM/VGG损失都是 需要输入的参考图像作为约束的损失。因此这一部分的UNet训练也是需要LOL数据集的，只是对抗损失网络是使用其他不成对的数据训练的。 ​ 损失函数：</p>
<p><span class="math display">\[\begin{aligned}
\left\{w_{1}, w_{2}, w_{3}\right\} &amp;=F_{\mathrm{RC}}\left(\left\{\Delta \hat{x}_{s_{1}}^{T}, \Delta \hat{x}_{s_{2}}^{T}, \Delta \hat{x}_{s_{3}}^{T}\right\}\right) \\
\hat{x}_{3}^{F} &amp;=\sum_{i=1}^{3} w_{i} \Delta \hat{x}_{s_{i}}^{T} \\
\Delta \hat{x}_{s_{i}}^{T} &amp;=\hat{x}_{s_{i}}^{T}-F_{\mathrm{U}}\left(\hat{x}_{s_{i-1}}^{T}\right), i=2,3 \\
\Delta \hat{x}_{s_{1}}^{T} &amp;=\hat{x}_{s_{1}}^{T}
\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
L_{\text {Detail }} &amp;=-\phi\left(\hat{x}_{3}^{F}-x\right) \\
L_{\text {Percept }} &amp;=\left\|F_{\mathrm{P}}\left(\hat{x}_{3}^{F}\right)-F_{\mathrm{P}}(x)\right\|_{2}^{2} \\
L_{\text {Quality }} &amp;=-\log D\left(\hat{x}_{3}^{F}\right)
\end{aligned}\]</span></p>
<p>其中 D 是 估计 生成图像符合人眼视觉效果的概率，这个D是在</p>
<blockquote>
<p>high-quality images selected from aesthetic visual analysis dataset</p>
</blockquote>
<p>这个数据集上训练的。Fp是VGG损失。</p>
<h2 id="实验">实验</h2>
<h3 id="训练细节">训练细节</h3>
<p>首先使用 LOL 上成对的数据 先训练第一阶段的网络，然后固定第一阶段 的权重 再训练第二阶段。具体训练方法如下：</p>
<figure>
<img src="https://cdn.jsdelivr.net/gh/changruowang/cloudimg/img/20210424120152.png" alt="" /><figcaption>1598841253259</figcaption>
</figure>
<h3 id="实验结果">实验结果</h3>
<figure>
<img src="https://cdn.jsdelivr.net/gh/changruowang/cloudimg/img/20210424120157.png" alt="" /><figcaption>1598841560362</figcaption>
</figure>
<ol start="3" type="1">
<li></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://xxxxx.com/posts/15395/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="常若望">
      <meta itemprop="description" content="可乐">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ruowang's blogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/posts/15395/" class="post-title-link" itemprop="url">Deep Retinex Net</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-05-11 10:10:30" itemprop="dateCreated datePublished" datetime="2021-05-11T10:10:30+08:00">2021-05-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-08-28 10:39:17" itemprop="dateModified" datetime="2021-08-28T10:39:17+08:00">2021-08-28</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%9B%BE%E5%83%8F%E9%A2%84%E5%A4%84%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">图像预处理</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%9B%BE%E5%83%8F%E9%A2%84%E5%A4%84%E7%90%86/%E5%9B%BE%E5%83%8F%E8%A7%86%E9%A2%91%E5%A2%9E%E5%BC%BA/" itemprop="url" rel="index"><span itemprop="name">图像视频增强</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>
<h2 id="主要贡献">主要贡献</h2>
<ul>
<li>建立了一个 采集与真实场景的 含有成对的低光照/正常光照的大尺度数据集（包含了合成和非合成的图像）</li>
<li>提出了一个基于retinex理论的图像分解的深度神经网络模型。这个模型是端到端训练的。</li>
<li>提出了用于学习图像分解模型的结构加权总变分损失。能够较好的平滑光照图同时保留原本的结构</li>
</ul>
<h2 id="网络结构">网络结构</h2>
<figure>
<img src="https://cdn.jsdelivr.net/gh/changruowang/cloudimg/img/20210424120111.png" alt="" /><figcaption>1572790251537</figcaption>
</figure>
<p>​ 可以看出，整个模型由序列的两部分组成，图像分解，和 去噪以及亮度多尺度调整</p>
<h3 id="数据驱动的图像分解-decom-net">数据驱动的图像分解 Decom-Net</h3>
<p>​ 这一部分将输入图像 S 分解为光照估计 I 和反射分量 R。一种分解反射和光照的方法是使用人工约束条件。然而根据retinex模型的那个式子很难实现，作者使用数据驱动的方式构建一个 Decom-Net来完成。这个网络的特点是损失函数的设计。由上图看出DecomNet的训练不需要低光照图的R和L来直接构建损失，而是通过以下几个约束条件来间接构建损失函数。</p>
<ul>
<li><p>低光照图的反射率图和正常光照的反射率图应该尽可能一样： <span class="math display">\[
\mathcal{L}_{i r}=\left\|R_{l o w}-R_{n o r m a l}\right\|_{1}
\]</span></p></li>
<li><p>应该可以根据R, I 还原重建出 I 对应的 S图 。假设Sn Sl分别为输入正常光照和低光照图像， Il In为分解产生的高低光照图 Rn Rl 为分解产生的对应的反射率。约束1可知Rn应该尽可能接近Rl。 约束2的意思则是： 使用Rn Il 可以还原出Sl 使用Rn In可以还原出Sn 使用Rl Il可以还原出Sl 使用Rl In可以还原出Sn 。 R*I 即根据Retinex理论重建的过程，而-则来度量还原的程度。</p></li>
</ul>
<p><span class="math display">\[
\mathcal{L}_{\text {recon}}=\sum_{i=l o w, n o r m a l} \sum_{j=l o w, n o r m a l} \lambda_{i j}\left\|R_{i} \circ I_{j}-S_{j}\right\|_{1}
\]</span></p>
<ul>
<li>结构平滑损失。即分解产生的光照图应该尽可能的平滑，因为认为一张图上的光照在各个区域是一致的。全变分(TV)损失，最小化整张图的梯度，通常在图像重建中作为先验平滑图像。但是直接应用TV损失来约束平滑光照图会在 图像本身梯度较大的区域失效 （下图黑边）。这是由于不管区域是纹理细节还是强边界，光照梯度都是均匀减少的。因此原本的TV损失对图像的结构是盲目的，如果在图像边缘进行强烈的模糊虽然产生的光照图会平滑但是反射率图会产生黑边。如下图。因此根据对应位置的反射率对光照梯度加上权重。上式 <span class="math inline">\(\nabla I_{i}\)</span> 表示微光图/正常光照图的梯度，一般的TV损失是直接对全图<span class="math inline">\(\nabla I\)</span> 求和作为损失来优化平滑图像。这里相当于根据反射率图像的梯度给梯度加上了一个权值exp()。在反射率图本身梯度大的地方权值小，小的地方权值大。作者认为在物理结构存在梯度的地方光照应该不连续 ？？</li>
</ul>
<p><span class="math display">\[
\mathcal{L}_{i s}=\sum_{i=l o w, n o r m a l}\left\|\nabla I_{i} \circ \exp \left(-\lambda_{g} \nabla R_{i}\right)\right\|
\]</span></p>
<ul>
<li>将上述三部分损失相加即为Decom-Net 采用的损失 <span class="math display">\[
\mathcal{L}=\mathcal{L}_{\text {recon}}+\lambda_{i r} \mathcal{L}_{i r}+\lambda_{i s} \mathcal{L}_{i s}
\]</span> 作者还提到 关于LIME网络也在计算光照图像时考虑到图像原本的结构，但是本质和自己的不同....</li>
</ul>
<blockquote>
<p>For LIME, the total variation constraint is weighted by an initial illumination map, which is the maximum intensity of each pixel in R, G and B channels. Our structure-aware smoothness loss instead is weighted by reflectance. The static initial estimation used in LIME may not depict the image structure as well as reflectance does, since reflectance is assumed as the physical property of an image.</p>
</blockquote>
<figure>
<img src="https://cdn.jsdelivr.net/gh/changruowang/cloudimg/img/20210424120115.png" alt="" /><figcaption>1572858053126</figcaption>
</figure>
<h3 id="多尺度光照调节-adjustment">多尺度光照调节 adjustment</h3>
<p>​ 这个adujust网络的结构类似于<strong>U-net</strong> 的编码解码结构。降采样块包含步长为2的卷积层，上采样块使用resize-convolution层，即先将特征图插值的方法上采样然后使用步长为1的卷积和激活。这一部分的损失即为调整过后的光照图和去噪过后的反射率图重建产生的图像和正常光照图像之间的l1损失。 ​ 可以看到，在这一部分还对反射率图进行了去噪处理，使用的时DMB3</p>
<h2 id="数据集">数据集</h2>
<p>​ 作者提出了一套数据集，包含两部分：成对的合成数据和真实的低光照图。由于真实场景的图像采集正常光照和低光照时会产生错位，作者采用了三步法来矫正。 ​ 合成图像，作者分析了已有的真实低光照图像和真实的正常图像光照的YCbCr中Y的分量，统计Y分量的直方图，根据他们的规律来合成低光照图像。</p>
<figure>
<img src="https://cdn.jsdelivr.net/gh/changruowang/cloudimg/img/20210424120119.png" alt="" /><figcaption>1572858105879</figcaption>
</figure>
<h2 id="结果">结果</h2>
<p>​ 本论文作者并没有贴出指标的定量分析，只是贴了几幅图。图像分解的方法是否有效有待考虑.... ​ 定性分析：</p>
<ol type="1">
<li><p>首先作者为了证明图像分解网络设计的有效性，贴出了和LIME分别对微光图和正常图像分解产生的R , I图，发现LIME在同一个图的低光和正常光图分解的R结果并不一致， 而R反应的应该是物体本身的折射率，在低/正常图下应该一样。</p></li>
<li><p>同样的，在R图中 由于去掉了光照分量，应该不存在阴影，而LIME的R中存在明显的阴影，但是RetinexNet除了有噪 声外是没有阴影的 说明光照分量取出的比较彻底。</p></li>
<li><p>文章所提的方法不会对局部区域过曝，主要是全局光照调节效果好</p></li>
<li><p>得益于加权TV损失，相对于基于DeHz去雾方法 结果中没有黑边</p></li>
<li><p>又对比了同样含有去噪方法的JED 网络，LIME(网络后去噪) 结果，发现Retinex的边缘细节得到较好的保留。</p>
<figure>
<img src="https://cdn.jsdelivr.net/gh/changruowang/cloudimg/img/20210424120123.png" alt="" /><figcaption>1572859097749</figcaption>
</figure></li>
</ol>
<p>感觉这种方法处理的结果</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://xxxxx.com/posts/313/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="常若望">
      <meta itemprop="description" content="可乐">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ruowang's blogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/posts/313/" class="post-title-link" itemprop="url">CURL</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-05-11 10:10:29" itemprop="dateCreated datePublished" datetime="2021-05-11T10:10:29+08:00">2021-05-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-08-27 17:00:31" itemprop="dateModified" datetime="2021-08-27T17:00:31+08:00">2021-08-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%9B%BE%E5%83%8F%E9%A2%84%E5%A4%84%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">图像预处理</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%9B%BE%E5%83%8F%E9%A2%84%E5%A4%84%E7%90%86/%E5%9B%BE%E5%83%8F%E8%A7%86%E9%A2%91%E5%A2%9E%E5%BC%BA/" itemprop="url" rel="index"><span itemprop="name">图像视频增强</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>
<h1 id="curl-neural-curve-layers-for-global-image-enhancement">CURL: Neural Curve Layers for Global Image Enhancement</h1>
<h2 id="主要贡献">主要贡献</h2>
<ul>
<li>基于多颜色空间的曲线调整块 CURL 神经修饰块。通过神经网络学习一条离散的点，代表一条曲线。使用该曲线分别在 Lab RGB 以及 HSV空间 对图像进行全局调整。</li>
<li>提出了多颜色空间损失函数 就是每个颜色空间调整完后的结果都有个损失函数约束</li>
<li>改进了U-NET的编解码结构，提出TED的backbone。降低参数量的同时提升了性能</li>
</ul>
<h2 id="主要内容">主要内容</h2>
<h3 id="主要结构">主要结构</h3>
<figure>
<img src="https://cdn.jsdelivr.net/gh/changruowang/cloudimg/img/20210424120017.png" alt="" /><figcaption>image-20210114110645797</figcaption>
</figure>
<p>主要流程从上图可以看出 首先使用一个编解码结构对输入图像处理，作者使用的是自己改进的TED结构做backbone。backbone输出一个特征图记为 F（多通道），取特征图 F 的前三通道为编解码器输出的RGB图像，后面的通道为特征。接着的CURL模块就是对这个RGB图做<strong>全局调整</strong>。首先将前三通道RGB图转为Lab格式，然后和剩余的特征图一起输入第一个块，输出得到一个向量（代表曲线），并使用这个曲线对Lab空间下的图调整后转为RGB格式 输入下一部分曲线调整模块。以此类推。</p>
<h3 id="ted">TED</h3>
<figure>
<img src="https://cdn.jsdelivr.net/gh/changruowang/cloudimg/img/20210424120021.png" alt="" /><figcaption>image-20210114111442957</figcaption>
</figure>
<p>TED的核心结构和UNET类似，右下角所示结构。和UNET的差别是，首先取消了UNET的每一层的跳跃连接，只保留了最高层的连接，并且这个连接作者称为 <code>MSCA-skip</code> 如图中红色的连线。MSCA结构是左边大图所示，有三个分支，最上面的分支是全局连接，有几个步长为2的卷积加最后一个全连接。中间分支是卷积率为2的分支，最下面是卷积率为4的分支。最后concat 1x1卷积压缩输出。输入输出特征图尺寸通道数一致。作者对比了他提出的这个TED 结构和 UNET的准确率参数量，可以看出参数量较少但是准确率还比较高。同时还看出只有第一层跨层连接参数量少很多但是效果不输全部都连接的情况。</p>
<p>另外，作者还讨论了两种输入模式，RGB-to-RGB, RAW-to-RGB 两种情况。对于RAW格式的输出，需要稍微欸修改backbone的编码结构，具体的就是使用 pixel-shuffle层将输入RAW格式 转化为 (H/r) (W/r) r^2 然后再输入bacnkbone的下采样路径，这样输出的尺寸是RGB的四分之一，再使用pixel shuffle的上采样方法获得和RGB输如一样大的特征图。RGB格式的输入就不使用 pixel shuffle操作了。RAW需要pixel-shuffle主要是因为RAW格式数据的特殊存储格式吧。</p>
<h3 id="curl模块">CURL模块</h3>
<figure>
<img src="https://cdn.jsdelivr.net/gh/changruowang/cloudimg/img/20210424120025.png" alt="" /><figcaption>image-20210114113514372</figcaption>
</figure>
<p>可以看出 前面 backbone输出的是一个 特征图 取特征图前3通道（蓝色部分为RGB图）曲线就是对这个进行全局调整的。神经网络的输出曲线是由全连接层产生，即离散的点。调整公式如下。 M代表输出点的个数，km代表第m个点的值，I 代表输入像素值，S代表输出的调整缩放因子。因此最终的调整结果为 S*I <strong>这样的另一个巧妙之处在于</strong>，例如HSV通道，可以使用 hue 对 saturation 进行调整（色相至饱和度曲线），换句话说缩放因子用 通道hue 计算，但是用算出来的缩放因子对saturation 通道调整。</p>
<blockquote>
<p>we arrange the neural curve layers in a particular sequence, adjusting firstly luminance and the a, b chrominance channels (using three curves respectively) in CIELab space. Afterwards, we adjust the red, green, blue channels (using three curves respectively) in RGB space. Lastly hue is scaled based on hue, saturation based on saturation, <strong>saturation based on hue</strong>, and value based on value (using four curves respectively) in HSV space</p>
</blockquote>
<figure>
<img src="https://cdn.jsdelivr.net/gh/changruowang/cloudimg/img/20210424120028.png" alt="" /><figcaption>image-20210114113729720</figcaption>
</figure>
<p>这个公式这么理解：</p>
<figure>
<img src="https://cdn.jsdelivr.net/gh/changruowang/cloudimg/img/20210424120032.png" alt="" /><figcaption>image-20210114115936548</figcaption>
</figure>
<h3 id="损失函数">损失函数</h3>
<p>上面的模块中涉及到 颜色空间的变换 作者使用的是 pytorch 可微分的变换实现。</p>
<figure>
<img src="https://cdn.jsdelivr.net/gh/changruowang/cloudimg/img/20210424120040.png" alt="" /><figcaption>image-20210114120611354</figcaption>
</figure>
<p><strong>为什么 要用乘积？</strong> S V乘积相同但是 相反表示不同深浅的颜色，但是损失依然为0？所以为啥要乘积相等？</p>
<figure>
<img src="https://cdn.jsdelivr.net/gh/changruowang/cloudimg/img/20210424120042.png" alt="" /><figcaption>image-20210114154837650</figcaption>
</figure>
<figure>
<img src="https://cdn.jsdelivr.net/gh/changruowang/cloudimg/img/20210424120043.png" alt="" /><figcaption>image-20210114154911861</figcaption>
</figure>
<figure>
<img src="https://cdn.jsdelivr.net/gh/changruowang/cloudimg/img/20210424120046.png" alt="" /><figcaption>image-20210114154919143</figcaption>
</figure>
<p>最后一个损失很明显是约束曲线 的斜率 不要有太大的突变 防止过拟合 即相邻点之间的直线的斜率差距不要太大。</p>
<h2 id="实验">实验</h2>
<p>作者在三个数据集上验证了算法</p>
<ul>
<li>Samsung S7 90张训练 10张测试 10张验证 包含RAW/RGB图像对</li>
<li>MIT-Adobe5k-DPE 5000张图，有专家调整的参考图 2250对训练图 500张测试 从训练集随机选择了500个做验证</li>
<li>MIT-Adobe5k-UPE</li>
</ul>
<h3 id="消融实验">消融实验</h3>
<h3 id="和其他方法对比">和其他方法对比</h3>
<figure>
<img src="https://cdn.jsdelivr.net/gh/changruowang/cloudimg/img/20210424120100.png" alt="" /><figcaption>image-20210114160519085</figcaption>
</figure>
<figure>
<img src="https://cdn.jsdelivr.net/gh/changruowang/cloudimg/img/20210424120104.png" alt="" /><figcaption>image-20210114160656667</figcaption>
</figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://xxxxx.com/posts/22637/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="常若望">
      <meta itemprop="description" content="可乐">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ruowang's blogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/posts/22637/" class="post-title-link" itemprop="url">ALEN_Attention-based</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-05-11 10:10:28" itemprop="dateCreated datePublished" datetime="2021-05-11T10:10:28+08:00">2021-05-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-08-27 17:00:31" itemprop="dateModified" datetime="2021-08-27T17:00:31+08:00">2021-08-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%9B%BE%E5%83%8F%E9%A2%84%E5%A4%84%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">图像预处理</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%9B%BE%E5%83%8F%E9%A2%84%E5%A4%84%E7%90%86/%E5%9B%BE%E5%83%8F%E8%A7%86%E9%A2%91%E5%A2%9E%E5%BC%BA/" itemprop="url" rel="index"><span itemprop="name">图像视频增强</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>
<p>Attention-based network for low-light image enhancement</p>
<h2 id="主要贡献">主要贡献</h2>
<ul>
<li>设计了一个端到端的应用于原始图像的增强网络，结合了 通道注意力 和 空间注意力模式，结合了局部和全局信息。</li>
<li>为了减少信息损失，设计了一个 ISL层 来 取代 max pooling</li>
<li>在SID数据集上评估了算法的有效性</li>
</ul>
<h2 id="non-local-operation">Non-local operation</h2>
<p>原始文献：Non-Local neural networks</p>
<p>​ 传统的卷积神经网络 其实只关注 局部图像的相关信息，而如果要获得全局信息，要通过很多层的卷积堆叠来扩大感受野，进而使网络形成全局信息的关注。全连接就是non-local的，而且是global的。但是全连接带来了大量的参数，给优化带来困难。基于此，作者根据 传统计算机视觉方法中的 非局部均值去噪滤波 的思想，设计了应用于CNN的 non-local操作。</p>
<ul>
<li>non-local operations通过计算任意两个位置之间的交互直接捕捉远程依赖，而不用局限于相邻点，其相当于构造了一个和特征图谱尺寸一样大的卷积核, 从而可以维持更多信息。</li>
<li>non-local可以作为一个组件，和其它网络结构结合，用于其他视觉任务中</li>
</ul>
<h3 id="非局部均值去噪滤波">非局部均值去噪滤波</h3>
<p>​ 传统的均值滤波的方法是 取目标像素位置的 领域区域的所有像素均值作为该位置滤波后的结果。而非局部的特点就是根据根据该局部区域和全局区域的相似度 作为加权系数来 加权平局。具体的过程如下图</p>
<figure>
<img src="https://cdn.jsdelivr.net/gh/changruowang/cloudimg/img/20210424115930.png" alt="" /><figcaption>1590138706176</figcaption>
</figure>
<p>w(x,y)一般定义为一个与欧式距离(2范数)相关的函数，设x，y的邻域宏块的欧式距离为d。对于待求位置x处的输出滤波，取x领域的小block 在周围大区域上滑动计算相似度，例如y位置处的相似度 d=||block(x)-block(y)||/block_size；则y加权到x点的加权因子为 w(x,y)=exp(-(dxd / (hxh))) 这个式子将原本的距离d转化为了 0-1之间的一个加权因子w(x,y)。h为衰减因子，h越小，加权因子越小，则加权点对当前点的影响越小，一般边缘保持得好但是噪声会严重，反之则边缘保持差图像更加光滑。计算欧式距离时，有时会考虑周围点对中心点的影响，会利用核函数对欧式距离加权。加权矩阵W要归一化。参考链接：https://blog.csdn.net/qianhen123/article/details/81043217</p>
<h3 id="non-local-表达式">Non-local 表达式</h3>
<p><span class="math display">\[\mathrm{y}_{i}=\frac{1}{\mathcal{C}(\mathrm{x})} \sum_{\forall j} f\left(\mathrm{x}_{i}, \mathrm{x}_{j}\right) g\left(\mathrm{x}_{j}\right)\]</span></p>
<p>​ 上面的公式中，输入是x，输出是y，i和j分别代表输入的某个空间位置，x_i是一个向量，维数跟x的channel数一样，f是一个计算任意两点相似关系的函数，g是一个映射函数，将一个点映射成一个向量，可以看成是计算一个点的特征。也就是说，为了计算输出层的一个点，需要将输入的每个点都考虑一遍，而且考虑的方式很像attention：输出的某个点在原图上的attention，而mask则是相似性给出。参看下图</p>
<figure>
<img src="https://cdn.jsdelivr.net/gh/changruowang/cloudimg/img/20210424115941.png" alt="" /><figcaption>1590139796102</figcaption>
</figure>
<figure>
<img src="https://cdn.jsdelivr.net/gh/changruowang/cloudimg/img/20210424115944.png" alt="" /><figcaption>1590139576348</figcaption>
</figure>
<p>为了简化问题，作者简单地设置g函数为一个1*1的卷积。相似性度量函数f的选择有多种。具体参考链接：</p>
<p>https://zhuanlan.zhihu.com/p/33345791 https://blog.csdn.net/shanglianlm/article/details/104371212</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#### pytorch  non-local实现</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NonLocalBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, channel</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(NonLocalBlock, self).__init__()</span><br><span class="line">        self.inter_channel = channel // <span class="number">2</span></span><br><span class="line">        self.conv_phi = nn.Conv2d(in_channels=channel, out_channels=self.inter_channel, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>,padding=<span class="number">0</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.conv_theta = nn.Conv2d(in_channels=channel, out_channels=self.inter_channel, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.conv_g = nn.Conv2d(in_channels=channel, out_channels=self.inter_channel, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.softmax = nn.Softmax(dim=<span class="number">1</span>)</span><br><span class="line">        self.conv_mask = nn.Conv2d(in_channels=self.inter_channel, out_channels=channel, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># [N, C, H , W]</span></span><br><span class="line">        b, c, h, w = x.size()</span><br><span class="line">        <span class="comment"># [N, C/2, H * W]</span></span><br><span class="line">        x_phi = self.conv_phi(x).view(b, c, -<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># [N, H * W, C/2]</span></span><br><span class="line">        x_theta = self.conv_theta(x).view(b, c, -<span class="number">1</span>).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>).contiguous()</span><br><span class="line">        x_g = self.conv_g(x).view(b, c, -<span class="number">1</span>).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>).contiguous()</span><br><span class="line">        <span class="comment"># [N, H * W, H * W]</span></span><br><span class="line">        mul_theta_phi = torch.matmul(x_theta, x_phi)</span><br><span class="line">        mul_theta_phi = self.softmax(mul_theta_phi)</span><br><span class="line">        <span class="comment"># [N, H * W, C/2]</span></span><br><span class="line">        mul_theta_phi_g = torch.matmul(mul_theta_phi, x_g)</span><br><span class="line">        <span class="comment"># [N, C/2, H, W]</span></span><br><span class="line">        mul_theta_phi_g = mul_theta_phi_g.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>).contiguous().view(b,self.inter_channel, h, w)</span><br><span class="line">        <span class="comment"># [N, C, H , W]</span></span><br><span class="line">        mask = self.conv_mask(mul_theta_phi_g)</span><br><span class="line">        out = mask + x</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    model = NonLocalBlock(channel=<span class="number">16</span>)</span><br><span class="line">    <span class="built_in">print</span>(model)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">input</span> = torch.randn(<span class="number">1</span>, <span class="number">16</span>, <span class="number">64</span>, <span class="number">64</span>)</span><br><span class="line">    out = model(<span class="built_in">input</span>)</span><br><span class="line">    <span class="built_in">print</span>(out.shape)</span><br></pre></td></tr></table></figure>
<h2 id="主要方法">主要方法</h2>
<h3 id="网络结构">网络结构</h3>
<figure>
<img src="https://cdn.jsdelivr.net/gh/changruowang/cloudimg/img/20210424115953.png" alt="" /><figcaption>1590137632181</figcaption>
</figure>
<figure>
<img src="https://cdn.jsdelivr.net/gh/changruowang/cloudimg/img/20210424115957.png" alt="" /><figcaption>1590137855616</figcaption>
</figure>
<h3 id="inverted-shufﬂe-layer">Inverted Shufﬂe Layer</h3>
<p>​ 作者说 设计这个的目的是因为常规的max pooling 虽然可以将低计算量但是会损失较多的信息，因此作者将 shuffle的思想应用到这个设计中。文章没有具体描绘结构 只是文字表述了一下，没懂到底怎样的结构</p>
<blockquote>
<p>Inspired by pixel shufﬂe in [18], we proposed a new pooling operation, named ISL, which includes inverted shufﬂe and convolution operation. After an inverted shufﬂe operation, the size of the feature map reduces to half of the original and the number of channels quadruples. Convolution layer with 1×1 kernels is performed after the inverted shufﬂe, which plays a role in selecting useful information while compressing the number of channels. In general, ISL not only has the effect of reducing the computation as a pooling layer but also makes the network more ﬂexible to select features.</p>
</blockquote>
<h3 id="损失函数">损失函数</h3>
<p>就是采用常用的 SSIM 和 L2损失结合。</p>
<h2 id="实验结果">实验结果</h2>
<h3 id="训练">训练</h3>
<p>这个文章可以算 learn to see in the dark 文章的补充，和它一样使用原始图像数据，只不过将注意机制等融入网络设计。训练集和测试集都是用的 SID。</p>
<h3 id="结果">结果</h3>
<figure>
<img src="https://cdn.jsdelivr.net/gh/changruowang/cloudimg/img/20210424120001.png" alt="" /><figcaption>1590140337600</figcaption>
</figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://xxxxx.com/posts/16507/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="常若望">
      <meta itemprop="description" content="可乐">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ruowang's blogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/posts/16507/" class="post-title-link" itemprop="url">Domain Adaptation for Image Dehazing</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-05-11 10:10:27" itemprop="dateCreated datePublished" datetime="2021-05-11T10:10:27+08:00">2021-05-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-08-27 17:00:31" itemprop="dateModified" datetime="2021-08-27T17:00:31+08:00">2021-08-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%9B%BE%E5%83%8F%E9%A2%84%E5%A4%84%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">图像预处理</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%9B%BE%E5%83%8F%E9%A2%84%E5%A4%84%E7%90%86/%E5%9B%BE%E5%83%8F%E8%A7%86%E9%A2%91%E5%8E%BB%E9%9B%BE/" itemprop="url" rel="index"><span itemprop="name">图像视频去雾</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://xxxxx.com/posts/55304/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="常若望">
      <meta itemprop="description" content="可乐">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ruowang's blogs">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/posts/55304/" class="post-title-link" itemprop="url">Briinging-old-photo</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-05-11 10:10:26" itemprop="dateCreated datePublished" datetime="2021-05-11T10:10:26+08:00">2021-05-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-08-27 17:00:31" itemprop="dateModified" datetime="2021-08-27T17:00:31+08:00">2021-08-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%9B%BE%E5%83%8F%E9%A2%84%E5%A4%84%E7%90%86/" itemprop="url" rel="index"><span itemprop="name">图像预处理</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%9B%BE%E5%83%8F%E9%A2%84%E5%A4%84%E7%90%86/%E5%9B%BE%E5%83%8F%E8%A7%86%E9%A2%91%E4%BF%AE%E5%A4%8D/" itemprop="url" rel="index"><span itemprop="name">图像视频修复</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <span id="more"></span>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/9/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><span class="page-number current">10</span><a class="page-number" href="/page/11/">11</a><span class="space">&hellip;</span><a class="page-number" href="/page/15/">15</a><a class="extend next" rel="next" href="/page/11/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

    <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 
      src="//music.163.com/outchain/player?type=0&id=885457449&auto=0&height=66">
    </iframe>

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">常若望</p>
  <div class="site-description" itemprop="description">可乐</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">141</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">24</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">36</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/changruowang" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;changruowang" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/changruowang@qq.com" title="E-Mail → changruowang@qq.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>

  <div id="sidebar-dimmer"></div>



      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        


<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">常若望</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
